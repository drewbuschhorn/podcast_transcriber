
import argparse
import os
import time
from typing import Dict
from dotenv import load_dotenv
import openai
import datetime
import glob
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)  # for exponential backoff
from pydub import AudioSegment
import logging
import torch
import whisper 
from whisper.utils import get_writer
from tqdm import tqdm

from helper import confirm_action, open_file_wdirs, sanitize_file_name

logging.basicConfig(level=args.loglevel)
load_dotenv()
openai.api_key = os.getenv('OPENAI_API_TOKEN')

@retry(wait=wait_random_exponential(min=2, max=60), stop=stop_after_attempt(6), reraise=True)
def transcription_with_backoff(**kwargs : Dict[str, str]):
  output_path = kwargs['output_dir'] + '/' + kwargs['filename']
  use_open_ai = kwargs.get('use_open_ai', True)
  try:
    transcriptions = []
    files = []
    count = 0
    format = 'srt' #'text' is the default but not timestamped.

    audio_clips = glob.glob(output_path + "/*.mp3")
    if len(audio_clips) <= 1:
      raise RuntimeError('No files found to transcribe.')

    if use_open_ai is False:
      print ("Not using OpenAI")
      torch.cuda.init()
      device = "cuda" # if torch.cuda.is_available() else "cpu"
      print ("Using device:", device)
      #load whisper model
      model_size = "medium.en"
      print("loading model :", model_size)
      model = whisper.load_model(model_size).to(device)
      print(model_size, "model loaded")

    with tqdm(total=len(audio_clips), desc="Transcribing", unit="clip") as pbar:
      for audio_clip_path in audio_clips:
        if os.path.exists(audio_clip_path):
          size = os.path.getsize(audio_clip_path)
          logging.debug(f'File exists: {audio_clip_path} size: {size}')
          if size < 1024:
            pbar.update(1)
            continue
        else:
          pbar.update(1)
          continue

        audio = AudioSegment.from_file(audio_clip_path, format='mp3')
        duration = len(audio) / 1000  # Convert milliseconds to seconds
        if duration < 0.15 or os.path.exists(audio_clip_path+'.'+format):
          pbar.update(1)
          continue

        if (use_open_ai):
          if count > 0 and count % 40 == 0:
            time.sleep(60)
            logging.info(f'Sleeping for 60 seconds to avoid OpenAI rate limits. Count: {count}')
          count += 1

          try:
            
            with open_file_wdirs(audio_clip_path, 'rb') as file:
              response = openai.Audio.transcribe(
                file=file,
                model="whisper-1",
                language="en",
                response_format=format
              )
            with open_file_wdirs(audio_clip_path +'.'+ format,'w',  encoding="utf-8") as out:
              out.write(response)
              pbar.update(1)
          except openai.error.InvalidRequestError as e:
            logging.error(f'InvalidRequestError: {e} + path: {audio_clip_path}')
            pbar.update(1)
            continue
        
        else:
          with torch.cuda.device(device):
            try:
              result = model.transcribe(audio_clip_path, fp16=True)
              writer = get_writer('srt', os.path.dirname(audio_clip_path))
              writer(result, audio_clip_path +'.'+ format)
              pbar.update(1)
            except Exception as e:
              logging.error(f'RuntimeError: {e}')
              raise e

  except Exception as e:
    logging.debug(e)
    raise e

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
                    prog='podcast_tracks_to_srts',
                    description='Sends the chunks of mp3 generated by my_parse.py' +
                                ' in output/[input_file_name]/*.mp3 to OpenAI whisper for' +
                                ' transcription to SRT format. Note that the SRT format' +
                                ' is frequently incorrect on timings and repetitions' +
                                ' from OpenAI.',
                    epilog='Run podcast_srts_to_transcript.py after completed')
    parser.add_argument('--file', metavar='-f', required=True,
                    help='file name of podcast file (to find working direction in output directoy)')
    parser.add_argument('--output', metavar='-o', default='output', required = False,
                    help='output directory to work in')
    parser.add_argument('--loglevel', metavar='-l', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                    default='INFO', help='Set the logging level')
    parser.add_argument('--uselocal', action='store_true',
                    help='Use OpenAI\'s whisper endpoint for transcription')
    args = parser.parse_args()
    logging.basicConfig(level=args.loglevel)
  
    use_open_ai = not args.uselocal
    if use_open_ai is True:
      openai.api_key = os.getenv('OPENAI_API_TOKEN')
      confirm_action('This action will load a large number of files to OpenAI\'s whisper endpoint'
                    + ' automatically. This may be slow and expensive. Continue? [Y/N]: ')
    else:
      pass
    transcription_with_backoff(filename = sanitize_file_name(args.file), output_dir = args.output, use_open_ai = use_open_ai)
    logging.info(transcription_with_backoff.retry.statistics)

    